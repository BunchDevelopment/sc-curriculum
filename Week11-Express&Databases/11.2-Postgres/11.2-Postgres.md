# `postgres`

## Relational Data

In the last lesson, we used `express` to build on our `lowdb`-based blog post API. While `lowdb` works for the simplest cases, it does not do well with _relational data_. Relational data is a way of modeling data through its relationships, organizing entity categories into a single structure (often "tables") with many properties ("columns") and instances of that entity ("rows") that is related to other entities through unique identifiers. This is a similar model to spreadsheets or other forms of columnar data presentation. By building our persistent data stores (or "databases") around this relational model, we can then enforce guarantees about those relationships, and we can query for subsets of entities through their relationships with one another more easily.

That's all a bit abstract, so let's take a look at a more concrete example using our existing `/posts` resource. At the moment, we only have one entity in our database (a `post`) which looks like this in JSON form:

```javascript
{
  "body": "This is a blog post",
  "id": "79fa42ee-1c92-4a34-b55a-d37b0a8bc2e0"
}
```

Every post is unique, and there are a number of properties that we could add that would continue to be unique to that blog post, e.g.:

```javascript
{
  "title": "Post Number 1",
  "body": "This is a blog post",
  "id": "79fa42ee-1c92-4a34-b55a-d37b0a8bc2e0",
  "published": 1597592298458
}
```

But what about including information about the author of a post? Let's include the `authorName` in each post, too:

```javascript
[
  {
    "authorName": "Test Testerson"
    "title": "Post Number 1",
    "body": "This is a blog post",
    "id": "79fa42ee-1c92-4a34-b55a-d37b0a8bc2e0",
    "published": 1597592298458
  },
  {
    "authorName": "Test Testerson",
    "title": "Post Number 2",
    "body": "This is a blog post",
    "id": "89fa42ee-1c92-4a34-b55a-d37bsldflk8c",
    "published": 1597592298458
  },
]
```

While this works, there's a problem: we probably want to include a lot more information about an Author than their name. What if they need to log in? Do they get a username and a password? What about an email address, or a _nom de plume_ versus a legal name? While we could include all of that information in every post that author contributes, maintaining that ever-changing collection of information inside the `post` entity would be a nightmare. Instead, we'd probably want to include a separate `author` resource that is then _referenced_ in the `post` resource. Something like:

```javascript
{
  "posts": [
    {
      "authorId": "79fee662u-1c92-4a34-b55a-d37675ikkC"
      "title": "Post Number 1",
      "body": "This is a blog post",
      "id": "79fa42ee-1c92-4a34-b55a-d37b0a8bc2e0",
      "published": 1597592298458
    },
    {
      "authorId": "79fee662u-1c92-4a34-b55a-d37675ikkC"
      "title": "Post Number 2",
      "body": "This is a blog post",
      "id": "89fa42ee-1c92-4a34-b55a-d37bsldflk8c",
      "published": 1597592298460
    },
  ],
  "authors": [
    {
      "id": "79fee662u-1c92-4a34-b55a-d37675ikkC",
      "name": "Test Testerson",
      "email": "test@testerson.com"
    }
  ]
}
```

We now have a relationship between _posts_ and _authors_. This is a good thing! Now we can make sure that author records can be as rich as they need to be without affecting the ergonomics of posts, and we can better adhere to the RESTful principle of Unique Resource Identifiers by exposing separate `posts` and `authors` endpoints. Let's re-organize our API to include an `authors` resource as well!

## Activity 1

### Separate Routers

1. First, let's create a new `server/routers` subdirectory to hold all of our routers. Inside of that directory, create a `server/routers/posts.js` file.

2. Instead of spec-ing routes on the `app` (which is a single, global routing Object), we should use the `express.Router` Object to reorganize the `/posts` routes as middleware for our global `app`. To do that, we'll `require` the `Router` from `express`, construct a new one in the `posts` file, and copy and paste all of the `posts` routing into our ew `posts.js` subbing `router` for every `app` instance. In the end, your `posts.js` file should look like this:

```javascript
const { Router } = require('express')
const router = Router()


router
  .route("/") // routes to posts will occur at the app-level!
  .get((_request, response) => {
    const posts = db.get("posts").value();

    response.json(posts);
  })
  .post((request, response) => {
    const post = db.get("posts").insert(request.body).write();

    response.json(post);
  });

router
  .route("/:id") // equivalent to /posts/:id
  .get((request, response) => {
    const id = request.params.id;
    const post = db.get("posts").getById(id).value();

    if (post) {
      response.json(post);
    } else {
      response.status(404).json({ message: "Not Found" });
    }
  })
  .patch((request, response) => {
    const id = request.params.id;
    const post = db.get("posts").updateById(id, request.body).write();

    if (post) {
      response.json(post);
    } else {
      response.status(404).json({ message: "Not Found" });
    }
  })
  .delete((request, response) => {
    const id = request.params.id;
    const post = db.get("posts").removeById(id).write();

    if (post) {
      response.json(post);
    } else {
      response.status(404).json({ message: "Not Found" });
    }
  });

module.exports = router // don't forget to export the router!

```

3. Once the router for posts has been built, `use` that router in `server/index.js`:

```javascript
// all previous requires and configuration here

const posts = require('./routers/posts')

const PORT = process.env.PORT || 8675

// notice how our posts router is middleware itself!
app.use(bodyParser.json()).use(morgan('dev')).use('/posts', posts)

app.listen(PORT, () => console.log(`Listening on port ${PORT}`));
```

4. Now we can repeat the exact same process for authors! See if you can create and include an `authors` router that exposes the exact same routes as `/posts`

5. If done correctly, you should now be able to `curl -X POST -H 'Content-Type: application/json'` both `author` and `posts` data. Try creating a few authors and posts with the schema outlined above in JSON format!

6. We won't try this with the current refactor, but think about the following: what would you expect to be returned from an `/author/:id/posts` route? How would we return that data?

## Constraints and Validation

Our simple database works well enough for simple cases and small prototypes, but there's a problem: how do we enforce the consistency of our data? What do you think will happen in the followig cases?

1. What if we set the `authorId` of a post to `null`? Does that make sense?
2. What happens if we set the `authorId` to an author that doesn't exist?
3. What hapens if we _delete_ an author with posts (leaving the post authorId in place)?

All of these are cases where the data model is invalidated by changing requirements. The burden of maintaining up-to-date data then becomes the burden of the application programmers (which doesn't make much sense). Instead, we can offload that maintenance burden to our database software through the use of _constraints_. Using a robust relational database, we can make sure that all of the above cases are _impossible_. And by making those cases impossible, it becomes much easier to query our data quickly (since we can assume complete and accurate data for all of the above cases).

The relational database that we'll be using in this course is called Postgres. Let's give it a look!


## Activity 2

### Postgres

0. Before we begin, make sure that you've [downloaded Postgres for your operating system](https://www.postgresql.org/download/) and installed it for your operating system.

1. Postgres works as a "daemonized" process. This means that it's always running in the background on a particular port. In the case of Postgres, it's almost always `5432`. To check that the `postgres` daemon is running, try the following:

  ```bash
  psql -U postgres -d postgres
  ```

  This should open up the postgres REPL for making queries against the default database (`postgres`) with a default user (also called `postgres`). You can inspect your existing database _tables_ (the `postgres` term for a resource type like `posts` or `authors`) with the shortcut `\d`. You should get back a response that says 'Did not find any relations'.

2. So now that we have an empty database, let's create some! To do that, we need to use the `CREATE TABLE` command, which will look like this for our posts and authors:

  ```sql
  CREATE TABLE authors (
    id SERIAL PRIMARY KEY,
    name TEXT NOT NULL,
    email TEXT NOT NULL
  );

  CREATE TABLE posts (
    id SERIAL PRIMARY KEY,
    title TEXT NOT NULL,
    body TEXT NOT NULL,
    author_id INTEGER REFERENCES authors NOT NULL
  );
  ```

3. That was a lot to take in all at once, so let's go over what we just did. First, we created two tables with a set of _columns_. Think of columns like you would in a spreadsheet, but, in this case, validated by types and constrained by null checks. These types and constraints make it impossible to do things like delete an author if they still have posts, or create a post without a title, or create a post with an `author_id` that doesn't exist. Pretty cool, right?

4. Recall that we had previously mapped CRUD operations to HTTP verbs (where CREATE is POST or PUT, READ is GET, UPDATE is PATCH or PUT, and DELETE is DELETE). We have similar commands in SQL (and, thus, in Postgres). Let's start by "reading" our authors with `SELECT`. Try the following:

  ```sql
  SELECT * FROM authors;
  ```
  
  `SELECT` is the "read" command in SQL, and `*` is like a glob pattern that means "select _all_ of the columns in this table". This is called a "query". You'll notice that this query returned zero _rows_... which makes sense, since there are no authors in our database.

5. Let's create an author with `INSERT`. `INSERT` syntax looks like this:

  ```sql
  INSERT INTO authors (name, email) VALUES ('Test Testerson', 'test@test.com');
  ```
  
  Notice that we need to explicitly map _columns_ to _values_. All text in SQL is surrounded by quotation marks (like we see with 'Test Testerson'), so we know that that these values satisfy the `TEXT` type requirements of the `name` and `email` columns. You'll also notice that we did't need to include `id`; this is because `id` is of type `SERIAL`, which is an auto-incrementing counter of integer values that's generated for us whenever a new `author` row is inserted into the table.

  Try the previous `SELECT` query again to see what happens now! Be sure to remember the `id` that comes back from that query.

6. Imagine for a moment that we wanted to update that author's email address. How might we do it? The SQL command here is called `UPDATE`, and it looks like this:

  ```sql
  UPDATE authors SET email = new.email@test.com WHERE id = $1;
  ```

  ...where `$1` is the id of the newly-created author. Notice that you can't possibly know that id ahead-of-time (even if it's easy to predict in this case).

  The `WHERE` clause is important in this case, as we only want to `UPDATE` a single row. Think of `WHERE` like a kind of `filter()` on a collection: the query is only evaluated for those rows that pass the predicate (or predicates) listed in the query's associated `WHERE` clause. A common pattern is the use of a unique `id` to "filter" possible rows down to a single row.

7. Similar to `UPDATE` we can also `DELETE` a single row with the following:

  ```sql
  DELETE FROM authors WHERE id = $1;
  ```

  ...again using the original `author` id.

8. Use `psql` to add at least author back to the `author` table. Notice how the original `id` value is never repeated!

9. Finally, make sure that you've created some posts with a similar `INSERT` statement. Notice that the `author_id` _must_ be a valid id in the `authors` table! If not, then `psql` prevents `INSERT`.

## `node-postgres` and Connection Pools

So now we have data in our local postgres database, but how can we actually use that data in Node? To do so, we'll use the [`node-postgres`](https://node-postgres.com/) (or `pg`) node module. Before we do that, though, we need to learn a bit more about database connections.

Recall that RESTful APIs could _not_ be "stateful": every request had to carry any "state" around in its headers or body, and every "response" had to do the same. Databases, though, are not RESTful. They carry state with a series of requests (i.e. "queries", as we've seen so far) through a single connection, and limit the number of active connections that a database can have at one time. This poses a problem for a public API: if you have to reconnect to a database for every single request, those initial connections will be slow (and resource-intensive), and most active APIs will start requesting more connections than Postgres is often capable of providing. The solution, then, is to use a _connection pool_ like the one provided by `pg`. Connection pools start a few connections, then share them through a single Object in a Node process. This means that database-hitting requests are both faster and less taxing on the database itself. As a bonus, most of these connection pools provide a simple way of passing around a database client that can be used to make these queries. Let's see how that will work with `node-postgres` and our `express` API.

## Activity 4

### `express` and `pg`

0. Start by installing `pg` with `npm install pg`

1. Create a `db.js` file next to `index.js`.

2. The connection pool provided by `pg` needs to know how to connect to our local database. So let's provide a _connection string_ either through a `DATABASE_URL` environment variable (used with Heroku) or with an hard-coded value in `db.js`, like so:

  ```javascript
  const connectionString = process.env.DATABASE_URL || 'postgres://postgres@postgres:5432'
  ```
  
  That connection string might look a little silly, but it contains a lot of useful information. `postgres://` is the protocol, similar to `http://` in the browser. The first `postgres` is the user (same as the `-U` flag in `psql`) and the `postgres` after the `@` is the database (same as the `-d` flag in `psql`). As mentioned before, `5432` is the default port for Postgres.

3. Now we can create a connection pool for our `app`, like so:

  ```javascript
  // keep all of the previous configuration and imports
  const { Pool } = require('pg')

  const connectionString = process.env.DATABASE_URL || 'postgres://postgres@postgres:5432'
  const pool = new Pool({ connectionString })

  module.exports = pool
  ```

4. Let's try using postgres to `GET` all of our posts instead of using our `lowdb` database! In `/routers/posts.js`, try the following in the `/` route:

  ```javascript
  // all previous imports the same
  const db = require('../db')

  router
    .route("/posts")
    .get((_request, response) => {
      // notice the callback pattern!
      db.query("SELECT * FROM posts", (error, posts) => {
        if(error) {
          response.status(500).json({ error }) // send the SQL error if something goes wrong
        } else {
          response.json(posts)
        }
      });
    })

  ```
  
  While this is slightly more complex than our `lowdb` queries, we now have the ability to back our data with something a lot more powerful than a `db.json` file!

5. Finally, see if we can refactor every route with the Postgres version instead of the `lowdb` version of our CRUD operations. Those queries should be identical to the ones that we used in `psql`!

## Activity 4

### Deployment

Last but not least, we need to get a running version of Postgres with a similar schema up-and-running in our production environment. Heroku has a Postgres add-on that makes this very easy for small projects like ours!

0. To start, install the `heroku` command-line interface by copying this into your terminal:

  ```bash
  curl https://cli-assets.heroku.com/install.sh | sh
  ```

  Then login with `heroku login`

1. Once the CLI is installed, go to your Heroku app dashboard and click on the 'Resources' tab. In that tab, there's a search bar in the Add-Ons section. Type in `postgres` and select the `Heroku Postgres` option that appears.

2. After selecting Heroku Postgres, you should be able to see a new dashboard link appear for this resource as soon as it's ready.

3. Now that your database exists, you can `psql` into that database with `heroku pg:psql`. This is exactly like the `psql` we use locally, but is connected to your production database... so be careful!

4. Once you have a `psql` connection, you'll notice that your production database is just as empty as our local database was. Use the exact same table-creation commands that we used before to create `posts` and `authors` tables in your new database and to create a few posts and authors.

5. Once that is set up, push your project to `master` and watch Heroku handle your new postgres-backed deployment!

6. At this point, you should be able to replace any calls to `jsonplaceholder.typicode.com` in your SPA with your new public API. Nice work!

